\documentclass{MO_article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}

\title{The LFRic Infrastructure: Its design and use}
\author{The LFRic Team}

\date{\today}

\begin{document}

\maketitle

\section{Document Source}

Source text for this document is within the documents directory of the
LFRic Project and will be updated in line with changes to the
repository. PDF versions of this document and other LFRic PDF and Trac
documents will periodically be updated at the following location in
the Met Office Shared Repository System. An account and password is
required if external to the Met Office:

\begin{verbatim}
https://code.metoffice.gov.uk/trac/lfric/wiki/LFRicDocumentationPapers
\end{verbatim}

\section{Abstract}

This document provides an overview of the LFRic Infrastructure, and
how to use it to write code for earth system modelling
applications. The LFRic Infrastructure comprises Fortran code that
supports scientific applications written according to a software
architecture called \textbf{PSyKAl} (see Section~\ref{sec:concepts}
for a description). Some of the application's Fortran code is
generated by a separate program called \textbf{PSyclone}. PSyclone
generates the platform-specific code required to support the parallel
deployment and optimisation of the application. This document includes
an overview of PSyclone including a simple worked example. PSyclone
has its own comprehensive documentation \cite{psyclone}.

The LFRic Infrastructure underpins the development of a new atmosphere
model being developed to replace the Met Office Unified Model. The new
model will be based on the GungHo dynamical core \cite{gunghocs}
coupled to subgrid parametrizations, many of which are also used
within the Unified Model (UM). The new model is commonly referred to
as the \textbf{LFRic Atmosphere Model}.

At its core, the LFRic Infrastructure provides a parallel data model
to store and manipulate model fields. Scientists write algorithms that
represent the basic mathematics of the model, and kernels that
implement individual operations on subsets of model fields. Each
kernel must be written to the PSyKAl standard, which requires metadata
descriptions of its inputs and structure. The metadata enables
PSyclone to generate the appropriate LFRic code that parallelises and
optimises the whole model.

The PSyKAl design provides a separation of concerns between scientific
code and parallel code. It permits the same science application to be
run in serial, MPI or OpenMP parallel, or in mix-model parallel (both
MPI and OpenMP), all without changing the science code. Furthermore,
additional transformations can be applied to the parallel code to
include other optimisations such as reordering of code or fusing of
loops, or to support requirements of different compute platforms.

The LFRic Infrastructure supports both the mixed finite element scheme
of the GungHo \cite{gunghocs} dynamical core and the more
traditional finite difference schemes of the UM physics codes. GungHo
has been developed to run on meshes such as the cubed sphere or
diamond sphere. Therefore the LFRic Infrastructure needs to support a
range of generalised unstructured meshes.

This document aims to introduce a new reader to LFRic with an overview
of the PSyKAl design and a description of the main parts of a typical
LFRic application, followed by descriptions of some of the key
components.

In support of LFRic developments, there exists a build system and a test
system. These systems are not documented here -- refer to the LFRic
Trac wiki system.

At the time of writing (2020), the LFRic infrastructure is being
actively developed, so this document is likely to undergo frequent
updates. The latest version can always be built from a fresh
installation from head of the LFRic trunk. The document aims to
identify some of the areas which are likely to change due to ongoing
or anticipated developments.

\section{Introduction}

The document aims to provide a gentle introduction to LFRic going only
as deep into the technical details as is required to describe the
basics of the LFRic data model, the architecture and the make up of an
LFRic-based scientific application.

Section~\ref{sec:concepts} gives an overview of the novel requirements
of the LFRic model that are driven by its support for the GungHo
finite element formulation and the unstructured mesh, and defines some
terms. Section~\ref{sec:psykal} describes the PSyKAl architecture of
the core science code within the model, and illustrates how scientists
will typically contribute new science to existing models. Having
explained the high level architecture, Section~\ref{sec:miniapps}
moves on to describes the the broad structure of a typical LFRic-based
application. Code examples derived from LFRic atmosphere model
illustrate the descriptions.

Further technical documentation is under development which will
provide comprehensive descriptions of all of these parts of the
system \cite{lfric_wikidocs}.

\section{Key Concepts and Requirements\label{sec:concepts}}

\subsection{Overview of the architecture}

To meet the needs for more flexible parallel deployment of codes on
future HPC architectures, LFRic has implemented a library\footnote
{Currently, the set of functions required to create a true LFRic
library have not yet been sufficiently isolated from other more
scientific functions} of infrastructure that supports the PSyKAl
design and complements the PSyclone application. This section provides
an overview of the scientific and technical requirements of this
infrastructure library with a focus on handling of fields. It also
includes a description of several of the terms that have been brought
in with the finite element implementation that LFRic aims to support.

The LFRic Infrastructure incorporates the following:

\begin{itemize}
\item Functions that support creation, parallelisation
  and manipulation of model fields used in the mixed finite element scheme
  of the GungHo dynamical core, as well as finite difference fields.
\item Base classes for components of an LFRic application such as
  kernels and operators.
\item APIs to supporting infrastructure such as that for diagnostic
  output, clocks and calendars, and for log messages.
\end{itemize}

\begin{figure}
\centering
\resizebox{0.66\linewidth}{!}{
\includegraphics*{psykalWithFlow}}
\caption{Schematic of the LFRic design recommendation illustrating
  the PSyKAl design for the science ``single model'' code, the
  driver layer and supporting infrastructure.}
\label{fig:psykal}
\end{figure}

At the core of the LFRic design is the 3-layered PSyKAl design shown
in Figure~\ref{fig:psykal}. The separation of concerns between
scientific code and parallel code is delivered by separating the
``Parallel Systems'', or \textbf{PSy Layer} code from the higher-level
scientific ``Algorithm'' code which operates on global data from lower
level ``kernel'' code which operates on small chunks of the global
data. The PSy layer code breaks the global data down into chunks and
passes the chunks to the kernels, applying any requested parallel
computation strategies.

The appropriate method for ``chunking'' data depends on the operation
and the application. Basic arithmetic operations like addition and
multiplication need simply to loop over individual data points. Finite
element methods, however, operate on whole cells of data in one
operation: each cell can have more than one data point. For earth
system applications, a kernel can be written to operate on a range of
cells. For the GungHo dynamics, kernels are written to operate on a
vertical column of cells.

By using metadata to define kernel interfaces precisely, it is
possible to automatically generate PSy layer code, and PSyclone has
been developed to do this. PSyclone generates correct code which can
include transformations that apply a range of parallel strategies and
other optimisations. 

PSyclone is written to support other applications in addition to
LFRic. The specific standard that supports code generation for LFRic
applications is referred to (for historical reasons) as the Dynamo0.3
API. This API is documented in a section of the general PSyclone
documentation \cite{psyclone}.

% A section of a 3D mesh Mesh
\begin{figure}
\centering
\resizebox{0.66\linewidth}{!}{
 \includegraphics*{w2_w3_vert_3d}}
\caption{An illustration of two fields on the same mesh. The mesh has
  three cells in the horizontal and four vertical levels. On the left
  data points are on each face of the cell and on the right the data
  points are in the centre of the cell. In both cases, points are
  numbered consecutively in the vertical. The numbering scheme for the
  field on the left takes into account hidden faces between and behind
  the cells. For example, the four faces between the left and middle
  column of the left-hand figure could be numbered 5 to 8. The five
  faces on the bottom and top of the cells in the column of on the
  right would be numbered 51 to 55, but only the top face, 55, can be
  seen.}
\label{fig:Mesh}
\end{figure}

In support of the GungHo finite element method, the LFRic
Infrastructure can be used to create fields whose innermost index
comprises data points in vertical columns. Figure~\ref{fig:Mesh}
illustrates numbering of data points for two types of fields. 

While the finite element and finite difference methods are not the
same some useful analogies can be made. The right-hand
Figure~\ref{fig:Mesh} could be used to store fields such as density
which are situated on half-levels in the Charney-Phillips grid. The
left-hand field could be used for storing both staggered horizontal
winds as in the Arakawa C-grid, and vertical wind fields as in the
Charney-Phillips grid~\cite{umdp15}.

The looping strategy that PSyclone applies to such fields is to loop
over cells at the surface, passing references to all the data points
on this cell into the kernel. The kernel can loop over the individual
cells of the column by incrementing each of the data point references
to access the data for each subsequent cell. A worked example based on
Figure~\ref{fig:Mesh} is given in Section~\ref{sec:dofs}.

The LFRic infrastructure supports distributed memory domain
decomposition of fields. Field data accessed by the PSy layer is a
subdomain of the global field with any required halo data.

PSyclone can generate code that includes directives and calls to
support distributed memory parallel operations (MPI) such as halo
swaps, and shared-memory parallelism such as OpenMP parallel
loops. Algorithms can be written to group several kernel calls
together: by examining metadata in such a group of kernels PSyclone
can assess dependencies between kernels and generate correct code that
takes these dependencies into account. For example, it may be optimal
to compute one kernel deeper into the halo so as to remove a halo
swap that would otherwise be required to run the next kernel.

\subsection{Requirements summary}

The following lists some of the major requirements that need to be met
by the LFRic Infrastructure, including requirements relating to the
above issues. It should be noted that some of these requirements
underpin other complex requirements that are not described in detail,
such as the need for a comprehensive diagnostic system.

Where implementation is substantially incomplete, or where there are
notable omissions, the current status of the LFRic implementation in
meeting these requirements is also described. Otherwise, it can be
assumed that the requirement has been met to a certain degree, noting
that further enhancements in some capabilities are anticipated, but
not documented here.

\begin{itemize}
\item Support for fields on global meshes, rectangular limited area
  and bi-periodic meshes, and lateral boundary condition (LBC)
  meshes. In principle, LFRic can support any mesh. In practice, the
  global mesh support is currently limited to the cubed
  sphere. Support for LBC meshes is in development.
\item Support for fields on meshes made up of columns of cells which
  are quadrilateral in the horizontal.
\item Support for finite element function spaces with higher order
  functions. This means the library must allow configuration of and
  access to the basis functions for the field, and the ability to
  store field data comprising more than one data point on more than
  one mesh entity (vertices, edges, faces or cell volumes).
\item Support for finite difference scalar and vector
  fields. Currently this support is provided by using a subset of the
  finite element implementation.
\item Support for organising and grouping fields as required by
  applications such as the Unified Model. For example, support for
  collections of fields of different types and support for tiled
  fields.
\item Distributed memory support is required based on horizontal
  domain decomposition with support for halos and halo exchange
  operations. Currently, LFRic includes support for partitioning a
  global cubed-sphere mesh, a limited area mesh and a bi-periodic mesh
  into rectangular meshes.
\item Support is required for different looping strategies to enable
  an operation over all or a subset of data points or cells in a
  field. LFRic provides the ability for kernels to operate on columns
  of cells (and the data points they contain), or individual levels
  (for certain field types). PSyclone support for the latter is in
  development.
\item Fields require support for halos of any depth. Support is
  required for looping over data points or on data on cells up to
  different halo depths. Currently this is supported, but all fields
  in an application are partitioned with the same depth of halo even
  if a smaller halo is needed for some fields.
\item To allow for concurrent communication and computation, support
  for looping over data on cells within or outside so-called ``inner
  halos'' is required. This requirement and implementation is
  described in detail in a separate LFRic document~\cite{distmem_impl}.
\item To support kernels that operate on continuous fields (those in
  which data points are shared between neighbouring columns), support
  is required for ``colouring'' the mesh to ensure concurrent
  shared-memory operations on the same shared data point can be
  prevented.
\item As GungHo is a mixed finite element scheme and as existing
  physics schemes are finite difference schemes, support for 
  placing of different field types on the same mesh is required.
\item Support for fields on hierarchical meshes. Each mesh in the
  hierarchy covers the same geographical domain, but the resolution of
  the lower meshes is an integer multiple of the higher resolution
  mesh such that, for example, each cell in the lower resolution mesh
  contains $2 \times 2$ or $3 \times 3$ cells of the higher
  resolution mesh. The hierarchy supports multigrid solvers, and
  will enable easier transformation of data between different science
  schemes running at different resolutions.
\item Support for mapping stencils of various shapes and depths to
  allow operations on cell data that are dependent on data in nearby
  cells.
\item Support for output and input of field data to and from
  files, to underpin a requirement for reading and writing dumps,
  reading ancillary files and writing diagnostic data.
\item Support for basic infrastructure such as clocks and calendars
  for managing time-stepping and long runs, and for log messages to
  provide progress messages to output files.
\item Support for interfacing to external model couplers such as OASIS.
\end{itemize}

\subsection{Class diagrams}

The LFRic infrastructure uses object-oriented features of Fortran to
implement much of its design. Where appropriate, UML class diagrams
have been created to document the objects. The source for these
diagrams can be found in the {\tt uml} directory of the LFRic
documentation. The diagrams can be rendered using a tool called {\tt
  plantuml}.

\subsection{Mixed precision support}

LFRic aims to support mixed-precision applications. Therefore it is
important to consistently apply the correct kinds to variables and
literal constants. Examples that follow include precision definitions,
therefore a brief description is given here.

The options for kinds are stored as parameters in a central module and
these include a ``default'' kind for reals and integers referred to as
{\tt r\_def} and {\tt i\_def}. Within the code base will be seen
declarations and code such as the following:

\begin{verbatim}
    use constants_mod,          only: i_def, r_def
...
    integer(i_def),     intent(in) :: mesh_id, twod_mesh_id
...
    const = 0.125_r_def / dt
\end{verbatim}

The latter method for writing literal real numbers must be followed as
the alternative {\tt real(0.125, r\_def)} is not guaranteed to give the
same result.

\subsection{Definition of Meshes and mesh entities}

A \textbf{mesh} is a formal description of a grid used by a model, and
comprises locations of points (referred to as vertices or nodes) and
connectivity between points. An LFRic application reads in a 2D mesh
from a file. Figure~\ref{fig:cubesphere2x2} shows the connectivity of
a 2D mesh representing the surface of a cube comprising 6 faces each
with 2 by 2 faces.

LFRic computes a 3D mesh from the 2D mesh by extruding the 2D cells
into vertical columns of 3D cells, thereby constructing a set of
connected vertical layers.

This documentation references the following mesh entities of the 3D
mesh:

\begin{itemize}
\item Vertex or node -- A zero-dimensional point in space.
\item Edge -- A connection between two vertices. The vertices and
  edges define the mesh topology.
\item Face -- A connection between three or more vertices that defines
  a surface enclosed by three or more edges. Note that while LFRic
  infrastructure could be extended to support triangular cells,
  currently all LFRic applications use quadrilateral cells with 4
  vertices and 4 edges per face.
\item Cell volume -- A volume enclosed by a set of faces.
\end{itemize}

A 3D cell refers to both the cell volume and the entities (faces,
edges and vertices) that enclose the volume. Neigbouring cells can
share the faces, edges and vertices. For example, immediate neighbours
of a cubed-sphere mesh share a face, four edges and four vertices.

\begin{figure}
\centering
\resizebox{0.66\linewidth}{!}{
\includegraphics*{cubesphere_2x2}}
\caption{Representation of a 2D cubed-sphere mesh, with numbering of cells, 
  edges and vertices. Note that some vertices and edges are
  duplicated within the diagram. Duplicated vertices are given the same number. For
  duplicated edges, only one of the representations is numbered.}
\label{fig:cubesphere2x2}
\end{figure}

\subsection{Dofs, dof-maps and function spaces\label{sec:dofs}}

The LFRic Infrastructure supports finite difference (FD), finite
volume (FV) and finite element methods (FEM). Simply put, in FV and
FEM, the value of a field at any point within a cell is computed as
the sum of one or more ~\textbf{basis functions} each multiplied by a
different data point. A basis function is a spatial function whose
value varies within the cell but is zero outside the cell. In FV and
FEM terminology, each data point is referred to as a ~\textbf{degree
  of freedom} or \textbf{dof}.

The representation of the field as a combination of dofs and basis
functions is referred to as a \textbf{function space}. Formally, the
value of a field $f$ at point $x,y,z$ within a cell\footnote{In an
  unstructured mesh, cells can be of different sizes or be
  distorted. Therefore, the location $x, y, z$ will be a relative
  location based on an ideal reference cell.} can be given as:

\[ f(x,y,z) = \sum_{i=1}^n x_i \sigma_i(x,y,z) \] 

Here, $n$ is the number of basis functions, $\sigma_n$ are the basis
functions, $x_n$ are the values of the degrees of
freedom. Figure~\ref{fig:basis_function} illustrates how field values
are computed from the dofs and function spaces for a simple
one-dimensional field with three basis functions. A characteristic
of a set of basis functions is that at the location of each dof, the
basis function associated with the dof is unity, and all the other
basis functions are zero. Therefore, the value of the field at the
location of the dof is the value of the dof itself.

Every cell in a given field has the same set of basis functions and
the same number of dofs in the same locations. A different field may
have a different function space and therefore a different set of basis
functions, and dofs at different locations.

\begin{figure}
\centering

\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{basisOrder2}
  \caption*{a) Basis functions}
\end{minipage}%
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{basisOrder2_dofs}
  \caption*{b) Degrees of freedom}
\end{minipage}

\begin{minipage}{0.7\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{basisOrder2_field}
  \caption*{c) Field value superimposed on the dof values from Figure b)}
\end{minipage}
\caption{Figure a) shows a set of basis functions (labelled A, B and
  C) for a 1D function space within a single cell that spans 0 to 1 on
  the horizontal axis. The dashed vertical lines marks the centre of
  the cell and the right-hand border of the cell. Figure b) shows the
  nominal location of three degrees of freedom (dofs). Multiplying
  each basis function in figure a) by the value of its related dof in
  Figure b) and summing the result gives a total displayed by Figure
  c). Note that the sum intersects all three dofs. This is because
  each dof location is located where its related function is 1 and the
  other two functions are zero.}

\label{fig:basis_function}
\end{figure}

If a dof exists on a face, edge or vertex, then it is shared between
the cells that share that face, edge or vertex. Where dofs are shared
between cells, the field is continuous across the boundary between the
cells because the value of the field at the dof is the value of the
dof itself. As illustrated in Figure a) of
Figure~\ref{fig:differentContinuity1D}, when approaching the dof
location from each side of the boundary between the cells, the value
of the field tends towards the same dof value.

In Figure b), however, the dofs are not shared between cells. While
they are at the same location as the edge, in this figure they are
shown slightly either side of the edge to illustrate the fact that
they are owned by one cell rather than being shared by two cells. As
the two dofs at the cells' boundary can have different values to each
other, it allows the field to be discontinuous.

The continuity of a function space is important when considering the
strategy for applying an operation to a field. A field which is
discontinuous can be computed by treating each cell in isolation. But
if a field is continuous, then the computation on neighbouring cells
influence the value of shared dofs. In this circumstance, the order in
which cells are computed can affect the value, and if shared memory
parallelism is to be applied, strategies are needed to prevent the
computation of two cells contending to write to the same dof at the
same time.

In two dimensions, a continuous horizontal wind field can be
represented by dofs located on faces between horizontally neighbouring
cells. This representation is somewhat analogous to the finite
difference C-grid representation of wind as a vector whose direction
is orthogonal to the edge that separates one C-grid cell from another.

To generate correct parallel code, PSyclone needs to know which
function spaces are continuous and which are discontinuous in the
horizontal. The continuity of each of the available function spaces is
explicitly declared in the Dynamo0.3 API.

\begin{figure}
\centering

\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{CG2_2D}
  \caption*{a) Continuous field}
  \label{fig:k0w3}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{DG2_2D}
  \caption*{b) Discontinuous field}
  \label{fig:k1w3}
\end{minipage}

\caption{A continuous and a discontinuous one-dimensional field,
  showing locations of dofs for each field. The vertical dashed line
  marks cell boundaries. The continuous field is described by dofs
  shared between cells, and so associated with the edge between the
  two cells. The dofs at the cell edges for the discontinuous field
  are at the same real location, but they are not shared with the
  neighbouring cell, and so are associated with the cell volume rather
  than its edge. In the diagram this is represented by placing the
  dots slightly away from the cell edge mark.}
\label{fig:differentContinuity1D}
\end{figure}

The LFRic infrastructure will support finite difference fields. While
such fields do not have basis functions, for simplicity this document
will use the ``dof'' term to describe data points in these fields
too.

\subsubsection{Looping over columns of dofs}

To compute changes to a finite element field within a cell, a kernel
will need to operate on all of the dofs in the cell. In a regular mesh
such as a lat-lon mesh it may be possible to compute the location of
all dofs belonging to a given cell based on the cell location. For a
general unstructured mesh, though, this is not feasible. Instead, for
each type of field, look-up tables are created that can be used to
reference all dofs in each cell. These look-up tables are referred to
as \textbf{dof-maps}. 

As noted in Section~\ref{sec:concepts} LFRic stores dof-maps for the
lowest cell of the 3D mesh as the dof-maps for the cells in the next
layer can be computed by incrementing the dof-map addresses. The
ability to infer the dofmaps of successive cells in a vertical column
enables kernels to be written that operate on a column of cells based
on the dof-map for just one cell.

The illustrative examples given so far considered 1-dimensional and
2-dimensional fields and function spaces. GungHo, however, uses
3-dimensional function spaces that require dofs in the full
3-dimensional space of the cell as illustrated by
Figure~\ref{fig:Mesh}, and used in the following example.

In the left-hand diagram of Figure~\ref{fig:Mesh}, each cell has 6
dofs: one per cell face, including some shared with another cell. Each
dof is related to one of 6 basis functions in this function space. The
dof-map for the whole field is a 2D array containing a row of 6 dof
addresses for each cell at the base of each column:

\[ dofmap\_face = \left\{ \begin{array}{cccccc} 
1 & 5 & 9 & 13 & 17 & 18 \\
22 & 26 & 30 & 5 & 34 & 35 \\
39 & 43 & 47 & 26 & 51 & 52 \\
\end{array} \right\} \]

The dof-map for the right, where the dofs are within the volume of
each cell, is simpler:

\[ dofmap\_volume = \left\{ \begin{array}{c} 
1 \\
5 \\
9 \\
\end{array} \right\} \]

A kernel can be called with a reference to the dof-map arrays for each
of the lines in the two dof-maps, and so can operate on the bottom
cell and the column of data above it. As discussed in
Section~\ref{sec:concepts} the kernel is called via the PSy layer code
which calls the kernel many times with different chunks of the global
data, and for a kernel, the appropriate chunk is all the data in a
vertical column of cells.

\subsubsection{A simplified kernel and PSy layer example\label{sec:simplePsy}}

Section~\ref{sec:psykal} describes the PSyKAl implementation in much
more detail using code snippets from the current LFRic codebase. Here,
a simpler implementation of the PSyKAl architecture is shown for a
kernel operating on fields on the two function spaces whose dof
locations are represented in Figure~\ref{fig:Mesh}: a field with data
on faces, {\tt face\_data}, and a field with data within the cell
volume, {\tt volume\_data}. Among other arguments, the kernel
subroutine interface could look like this:

\begin{verbatim}
    subroutine my_kernel_code(nlayers,                       &
      face_data, volume_data...                              &
      dofs_per_cell_face,   total_face_dofs,  dofmap_face    &
      dofs_per_cell_volume, total_volume_out, dofmap_volume, & 
      ...)
\end{verbatim}

The kernel arguments include the number of layers, the full data for
all the output and input fields, the dof-maps for the fields, and size
of the data and dof-map arrays. By convention, GungHo kernels list the
modified field first in the argument list, so in this case, the field
to be modified is the field on faces.

For such a kernel, the PSy layer code could look something like this:

\begin{verbatim}
  do cell = 1, ncells
    call my_kernel_code(nlayers, face_data(:), volume_data(:)...     &
        dofs_per_cell_face,total_face_dofs,dofmap_face(:,cell)       &
        dofs_per_cell_volume, total_volume_out, dofmap_volume(:,cell) 
                 )
  end do
\end{verbatim}

For the function spaces and mesh illustrated in Figure~\ref{fig:Mesh}
the array sizes and dof-map variables would be set as follows

\begin{verbatim}
 dofs_per_cell_face   = 6
 dofs_per_cell_volume = 1

 total_face_dofs   = 55
 total_volume_dofs = 12

! The face dofmap has a row for each column. Each row contains 6 dof addresses
 dofmap_face = reshape( [  1, 5,   9, 13, 17, 18,           &
                          22, 26, 30,  5, 34, 35,           &
                          39, 43, 47, 26, 51, 52 ], [6, 3] )

! The volume dofmap has a row for each column. Each row contains 1 dof address
 dofmap_volume  = reshape( [ 1,             &
                             5,             &
                             9 ], [ 1, 3 ] )
\end{verbatim}

Within the kernel, the data and dof-map arrays would be declared in
this way:

\begin{verbatim}
  ! Declare the input arrays using the input array sizes
  integer(kind=i_def), intent(in)  :: dofmap_face(dofs_per_cell_face)
  integer(kind=i_def), intent(in)  :: dofmap_volume(dofs_per_cell_volume)
  real(kind=r_def),    intent(in)  :: face_data(total_dofs_face)
  real(kind=r_def),    intent(in)  :: volume_data(total_dofs_volume)
\end{verbatim}

The field on faces is being modified in this kernel, based on the
input field of cell volume data. The loop over columns is managed by
the PSy layer, so the kernel needs to loop over layers (each cell in
the vertical column), and over both dof-maps.

\begin{verbatim}
  ! Loop over all layers
  do k = 0, nlayers - 1
    ! Loop over all the dofs in a single cell of the input field
    do dofs_in = 1, dofs_per_cell_volume
      do dofs_out = 1, dofs_per_cell_face
        ! Increment each output field dof as a function of the input field dof
        face_data(dofmap_face(dofs_out) + k) =                      &
                face_data(dofmap_face(dofs_out) + k)                &
                function_of(volume_data(dofmap_volume(dofs_in) + k))
    end do
  end do
\end{verbatim}

In the first call to the kernel ({\tt cell = 1}), the first iteration
of the loop over layers ({\tt k = 0}) uses the raw dof-map addresses
from the input field which address the dofs in the bottom left cell of
the mesh represented in Figure~\ref{fig:Mesh}: dof number 1 for the
input array and dofs 1, 5, 9, 13, 17 and 18 for the output array.

In the second iteration of the outer kernel loop, with {\tt k = 1},
the kernel adds 1 to all the dof-map addresses.  Doing this gets
references to the data points in the second cell of the column: dof
number 2 for the input fields and dofs numbered 2, 6, 10, 14, 18
and 19 for the output array.

The fourth iteration of the outer loop, the final set of dofs for the
output field are obtained by incrementing the dof-map twice more to
give dofs numbered 4 , 8, 12, 16, 20 and 21. As can be seen, dof 21 is
the dof on the top-most face of the uppermost cell of the column.

The second column of cells is operated on in the second call to the
kernel, with {\tt cell = 2}, with the input dof-map referencing dof
number 5 and the output dof-map referencing dofs 22, 26, 30, 5, 34 and
35.

As an aside, it should be noted that the dof-map for the left and
middle column both reference dof number 5. This indicates that dof 5
is situated on the face shared by the bottom left and bottom middle
cell. The sharing of dofs means that the kernel must be performing
some sort of increment of the output data rather than over-writing it,
which is what our simple kernel does. Otherwise the second call to the
kernel would over-write the results of the first. In fact, PSyclone
includes rules which will reject a kernel that seeks to write data to
a continuous field as opposed to incrementing data it already
contains.

\subsubsection{LFRic Function spaces and element
  order\label{sec:function_spaces_intro}}

This section introduces the LFRic function space object which
underpins fields used in LFRic. In previous sections it was noted that
the function space is a concept in the finite element method whereby
data and basis functions can define a field that spacially varies
within an individual cell, and this was illustrated for individual
cells in Section~\ref{sec:dofs}. The implementation of the function
space object in LFRic goes further than this as it maps all of the
data to the whole three-dimensional mesh.

LFRic supports a range of \textbf{function space types}. The function
space type is defined by the type of basis functions which considers
both the functions and the locations of the dofs. Each of the several
different function space types has particular layout of dofs within a
cell. For example, the function space with face centred dofs is
referred to as $\mathbb{W}_{2}$ and the cell-centred function space as
$\mathbb{W}_{3}$. Essentially, the number represents the number of
dimensions of the entity on which the dof is placed. Following this
pattern, then, the $\mathbb{W}_{0}$ function space has dofs on the
vertices and the $\mathbb{W}_{1}$ function space has dofs on edges of
the mesh.

This is not the full story. The above descriptions are for the
``lowest order'' versions of the four function spaces. At lowest
element order, the basis functions are a constant function (for
$\mathbb{W}_{3}$) or linear functions. Higher order function spaces
are possible for each type of function space. Each higher order
function space has more basis functions and therefore more dofs. The
basis functions are changed from being constant to linear for
$\mathbb{W}_{3}$, and from linear to quadratic for the other function
spaces.

Given all the above, the definition of a field requires both
definition of the function space type, the mesh (which defines the
horizontal domain and number of vertical levels) and the order of the
basis functions.

Figure~\ref{fig:k0k1w0-w3}, includes a representation of the dof
locations in a single cell for the four function spaces
$\mathbb{W}_{0}$ to $\mathbb{W}_{3}$ at lowest and next higher
order. Note that Figure~\ref{fig:k0k1w0-w3} e) and
Figure~\ref{fig:k0k1w0-w3} g) can be mapped to the two fields shown in
Figure~\ref{fig:Mesh}.

\begin{figure}
\centering

\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k0_W0_dofs}
  \captionof*{figure}{a) $\mathbb{W}_{0}, k = 0$}
%  \label{fig:k0w0}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k1_W0_dofs}
  \captionof*{figure}{b) $\mathbb{W}_{0}, k = 1$}
%  \label{fig:k0w02}
\end{minipage}

\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k0_W1_dofs}
  \captionof*{figure}{c) $\mathbb{W}_{1}, k = 0$}
%  \label{fig:k0w1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k1_W1_dofs_circ}
  \captionof*{figure}{d) $\mathbb{W}_{1}, k = 1$}
%  \label{fig:k1w1}
\end{minipage}

\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k0_W2_dofs}
  \captionof*{figure}{e) $\mathbb{W}_{2}, k = 0$}
%  \label{fig:k0w2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k1_W2_dofs_circ}
  \captionof*{figure}{f)$\mathbb{W}_{2}, k = 1$}
%  \label{fig:k1w2}
\end{minipage}

\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k0_W3_dofs}
  \captionof*{figure}{g) $\mathbb{W}_{3}, k = 0$}
%  \label{fig:k0w3}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{k1_W3_dofs}
  \captionof*{figure}{h) $\mathbb{W}_{3}, k = 1$}
%  \label{fig:k1w3}
\end{minipage}

\caption{Locations of dofs for spaces $\mathbb{W}_0$ to $\mathbb{W}_3$
  for lowest order in left column, and next lowest in right. These
  images are not precise enough to show the subtleties of dof
  location -- see Figure~\ref{fig:cornerk1w1w2} for an expanded view
  of dofs under the faint circles in d) and f)}
\label{fig:k0k1w0-w3}
\end{figure}

\begin{figure}
\centering

\begin{minipage}{0.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{corner_W1}
  \captionof*{figure}{a) $\mathbb{W}_{1}, k = 1$
  (figure~\ref{fig:k0k1w0-w3}d) expanded view \newline
  of just one corner}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{corner_W2}
  \captionof*{figure}{b) $\mathbb{W}_{2}, k = 1$
  (figure~\ref{fig:k0k1w0-w3}f) expanded view \newline
  of just one corner}
\end{minipage}

\caption{Details of the locations of dofs for the spaces
  $\mathbb{W}_1$ and $\mathbb{W}_2, k = 1$. The faint boxes and lines
  are, in reality, infinitesimally small but are drawn to allow
  the mesh entity on which the dofs actually sit on to be seen}
\label{fig:cornerk1w1w2}

\end{figure}

A kernel can be written to operate on fields of different order, and
the GungHo dynamics kernels are written this way. Running GungHo at
higher order is more expensive per kernel call, but the results should
be more accurate. A trade-off between order and model resolution can
be explored if higher order dynamics turns out to perform
proportionally better on certain compute architectures.

It is important to note that the continuity of a function space is the
same at both low and higher order. Given that the continuity can
affect the loop ranges and halo swaps required in the PSy layer, the
fact that continuity is unchanged means that the code does not have to
be regenerated to run the model at a different order.

As in Figure~\ref{fig:differentContinuity1D}, the dof locations shown
in Figure~\ref{fig:k0k1w0-w3} g) are slightly offset from the corner
to illustrate that the $\mathbb{W}_{3}$ function space is
discontinuous at higher order as well as at lowest order. It
distinguishes the function space from the lowest order
$\mathbb{W}_{0}$ function space in Figure~\ref{fig:k0k1w0-w3} a) which
is continuous. Figure~\ref{fig:cornerk1w1w2} shows similar subtleties
for the higher order version of the $\mathbb{W}_{1}$ and
$\mathbb{W}_{2}$ field. For example, one of the upwards pointing (red)
arrows in Figure~\ref{fig:cornerk1w1w2} a) is on the left-most edge of
the cell and two of them are on the cell face near to the cell
edge. This means the upwards component of the vector field represented
by this function space is continuous with horizontally neighbouring
cells but discontinuous across the boundary with vertical neighbours.

\section{Overview of the PSyKAl design\label{sec:psykal}}

This section provides a brief walkthrough of the PSyKAl design. It
uses code snippets from the current implementation of the GungHo
dynamics. There are similarities between these code snippets and those
used in Section~\ref{sec:simplePsy}. Additional details shown here
will reference other key aspects of the LFRic infrastructure and the
capabilities of PSyclone \cite{psyclone}.

The code-generation example includes references to some technical terms
used in the finite element method used by GungHo. The detailed meaning
of these technical terms is not too important for understanding the
PSyKAl design and the main principles of PSyclone code generation.

At the top-level of an LFRic application there is the model setup and
model configuration code, and a driver layer which calls the science
code to initialise the model and to iterate the model over
time-steps. Most of the science code exists within the PSyKAl layers
of the model below this top-level.

The PSyKAl design defines three of the layers of code that exist
within an LFRic application: Algorithms, Kernels and a parallel
systems layer or PSy layer. The design imposes a {\it separation of
concerns} between scientific aspects in the algorithm layer and the
kernel layer, and computational aspects such as OpenMP directives and
calls to the distributed memory library, in the PSy layer.

Algorithms aim to represent the mathematical operations that the
scientist will have written down in the equation set used to solve a
particular the numerical problem, with reference to kernels and other
operations that implement parts of the solution. Each kernel contains
the code that applies one of the numerical operations to a subset of
the model data. LFRic models use the PSyclone tool to generate PSy
layer code to interface between the algorithm and kernel code.


\subsection{Algorithm Code\label{sec:psykal:alg}}

Algorithms are Fortran subroutines. One algorithm can call one or more
other algorithms so providing support for a sensible organisation of
science code into a program of top-level algorithms that direct the
over-all flow of a time-step or a numerical operation, and lower-level
algorithms implementing specific parts of the numerical recipe.

Algorithms (and the driver layer) can create, initialise and destroy
fields. As noted previously, numerical operations on model fields
within algorithms are operations on the whole field and not on subsets
of the field. Operations on fields are represented as calls to a
non-existent subroutine called {\tt invoke}. The {\tt invoke}
subroutine call can have one or more argument. Each argument is either
a kernel type as written by a scientist or software engineer, or an
arithmetic or linear algebra operation which has support built into
the PSyclone API (see the Dynamo0.3 API description in the PSyclone
documentation \cite{psyclone} for a list of so-called
\textbf{built-in} operations). Each kernel or built-in operation takes
a set of arguments.

PSyclone parses the algorithm layer looking for calls to this
non-existent {\tt invoke} subroutine. For each such call, PSyclone
generates a separate subroutine which takes all the arguments of all
the kernels and built-ins found in the {\tt invoke} argument
list. PSyclone rewrites the algorithm code, replacing each {\tt
  invoke} call with a call to the generated subroutine.

The following example {\tt invoke} call requests two operations. The
first is a call to a kernel to compute a field of angular momentum
values referenced by the {\tt compute\_total\_aam\_kernel\_type}
argument.  The second is a summation of these values to return the
total angular momentum.

\label{page:invoke}
\begin{verbatim}
      ! Compute Total Axial Angular Momentum
      call invoke( name = "compute_axial_momentum",                   &
                 compute_total_aam_kernel_type(aam, u, rho, chi, qr), &
                 sum_X(total_aam, aam) )
\end{verbatim}

The kernel computes a field of angular momentum values {\tt aam} from
inputs of {\tt u} wind and {\tt rho} density. The {\tt chi} field is
the mesh coordinates. The {\tt qr} argument references the
\textbf{quadrature points}. In simple terms, these quadrature points
are a set of locations within the cell used to compute the result.

The summation operation {\tt sum\_X} references a PSyclone built-in
operation whereby PSyclone will generate the code that computes a
global sum of the field and stores it in the scalar real value {\tt
  total\_aam}.

This {\tt invoke} call includes an optional {\tt name} argument which
is used to name the subroutine that PSyclone generates. Using the {\tt
  name} argument can be helpful when debugging and profiling the code
as it makes it easier to find the relevant PSy layer subroutine. By
default, PSyclone will name the subroutine {\tt invoke\_} followed by
a number.

Given the above call, PSyclone will modify the algorithm code to
replace the call to the non-existent subroutine {\tt invoke} with a
call to a subroutine that will be generated by PSyclone whose
arguments are the combined arguments of all the original kernels and
built-ins of the original {\tt invoke} call. Note that arguments such
as {\tt aam} that appear in more than one kernel or built-in are not
repeated:

\begin{verbatim}
   call invoke_compute_axial_momentum(aam, u, rho, chi, total_aam, qr)
\end{verbatim}

This aspect of PSyclone code generation is relatively straightforward
as compared with the generation of PSy layer code required to call the
kernel. Therefore, before the PSy layer code is described, we first
describe key aspects of a kernel.

\subsection{Kernels}

Kernels are written by scientists and software engineers to operate on
a subset of a field each time they are called. In the example below,
the subset will be a single vertical column of data, which is the
first target for LFRic implementation.

The kernel module contains a subroutine for implementing the kernel
operation, and metadata that describes the kernel interface. The
kernel metadata must follow the strict LFRic metadata schema supported
by PSyclone Dynamo0.3 API.

As the kernel runs on just a subset of the model field, the role of
PSyclone (described in the next section) is to generate code that both
satisfies the interface to the kernel, and that loops over all the
data, calling the kernel with each subset. Together, this ensures
whole field is correctly operated on.

The kernel metadata is contained within a Fortran type definition that
extends an LFRic {\tt kernel\_type} base class. The kernel metadata
from the angular momentum kernel referenced by the above {\tt invoke}
is as follows:

\begin{verbatim}
  type, public, extends(kernel_type) :: compute_total_aam_kernel_type
    private
    type(arg_type) :: meta_args(4) = (/                &
         arg_type(GH_FIELD,   GH_REAL, GH_WRITE, W3),  &
         arg_type(GH_FIELD,   GH_REAL, GH_READ,  W2),  &
         arg_type(GH_FIELD,   GH_REAL, GH_READ,  W3),  &
         arg_type(GH_FIELD*3, GH_REAL, GH_READ,  WCHI) &
         /)
    type(func_type) :: meta_funcs(3) = (/              &
         func_type(W2,   GH_BASIS),                    &
         func_type(W3,   GH_BASIS),                    &
         func_type(WCHI, GH_BASIS, GH_DIFF_BASIS)      &
         /)
    integer :: operates_on = CELL_COLUMN
    integer :: gh_shape = GH_QUADRATURE_XYoZ
  contains
    procedure, nopass :: compute_total_aam_code
  end type
\end{verbatim}

Note, that while the metadata code is correct Fortran, its only
purpose is to describe the interface to PSyclone, and it does not
otherwise do anything at run-time. By storing metadata in such Fortran
constructs, we benefit from compiler checks to help spot invalid
metadata.

The components of the metadata definition above are as follows. As
noted above, the technical understanding of the different components
is not too important. The main aim is to show in the next section how
this metadata is used by PSyclone to generate correct code.

\begin{itemize}
\item A type definition defines a {\tt
    compute\_total\_aam\_kernel\_type} Fortran type which extends the
  LFRic Infrastructure {\tt kernel\_type} type.
\item The {\tt arg\_type} declaration constructs an array of {\tt
    arg\_type} objects each of which contains metadata for one of the
  kernel arguments. In this case, the first three arguments relate to
  the physical fields {\tt aam}, {\tt u} and {\tt rho}. The fourth is
  the coordinate field for the mesh, a 3-dimensional field called {\tt
    chi}. The {\tt GH\_WRITE} and {\tt GH\_READ} arguments describe
  whether kernel argument is an output or an input. The $\mathbb{W}_{chi}$,
  $\mathbb{W}_{2}$ and $\mathbb{W}_{3}$ references define the function
  spaces that the input fields are on.
\item The {\tt func\_type} defines what information about the argument
  field function spaces is required by the kernel. This kernel
  requires the finite element basis functions for all three function
  spaces used by the kernel, and the differential basis functions for
  the coordinate field only.
\item The {\tt operates\_on} integer is set to the entity over which
  the kernel needs to loop. For GungHo dynamics kernels this is always
  {\tt CELL\_COLUMN} meaning that each call to the kernel needs to be
  given a dof-map for all the dofs in the lowest cell of a column.
  PSyclone \textit{can} support other looping strategies.
\item The {\tt gh\_shape} integer references one of several quadrature
  rules defined within LFRic and PSyclone which tells PSyclone how
  many numbers and arrays it needs to extract from the quadrature object.
\item The procedure references the actual kernel code.
\end{itemize}

While the detailed meaning of the different function spaces is not
important for the illustration, in simple terms, in a standard GungHo
run: a $\mathbb{W}_{3}$ field is a cell-centred field typically used for mass
quantities; a $\mathbb{W}_{2}$ field has dofs on the cell faces and is used
for vector quantities such as fluxes and velocities. A $\mathbb{W}_{chi}$
field is a coordinate field representing the fixed coordinates of the
mesh.

The subroutine interface for the kernel is as follows. Clearly it has
many more arguments than the original {\tt invoke} call, and each
argument is described in Section~\ref{sec:psy_example} below. The
important thing to note is that all of the arguments and the order of
the arguments derive entirely from the kernel metadata. In fact, the
PSyclone toolset includes a stub generator {\tt genkernelstub} which
will generate the following subroutine call and all the argument
declarations based on the above metadata. See the PSyclone
documentation for details.

\begin{verbatim}
    subroutine compute_total_aam_code(                            &
                                      nlayers,                    &
                                      aam, u, rho,                & 
                                      chi_1, chi_2, chi_3,        &
                                      ndf_w3, undf_w3, map_w3,    &
                                      w3_basis,                   &
                                      ndf_w2, undf_w2, map_w2,    &
                                      w2_basis,                   &
                                      ndf_chi, undf_chi, map_chi, &
                                      chi_basis, chi_diff_basis,  &
                                      nqp_h, nqp_v, wqp_h, wqp_v  &
                                     )
\end{verbatim}

While the kernel metadata uses Fortran 2003 constructs, the
implementation of the kernel subroutine is Fortran 90 without
reference to LFRic data types or objects, and using only native scalar
and array variables.

The code for this kernel can be seen in the LFRic repository in the
kernels directory of the GungHo source code, so only a brief
description of its structure is given. In common with all other GungHo
kernels, the kernel loops over a vertical column of {\tt nlayer}
cells. The column it operates on is defined by the {\tt map\_\*}
arguments which map to the dofs associated with the cell at the bottom
layer of one of the columns. The raw map data addresses the
bottom-layer cell. Maps for successive cells going upwards in the
column are obtained by successively incrementing all the dof addresses
in the original map as can be seen by examining the dof numbering in
Figure~\ref{fig:Mesh}.

\subsection{The PSy layer code}
\subsubsection{PSyclone overview}
This section provides a brief overview of the general operation and
use of PSyclone, before describing in more detail the PSy layer code
that could be generated in support of the {\tt invoke} call and kernel
metadata described in the preceding sections. As noted elsewhere,
PSyclone has its own comprehensive documentation \cite{psyclone}.

As noted previously, PSyclone supports several different APIs
including another API for a simple ocean model. The specific API used
by LFRic is called the Dynamo0.3 API. The API is related to both the
LFRic metadata and the choice of LFRic built-in operations
available. The Dynamo0.3 API is fully-documented in the PSyclone
documentation.

By default, PSyclone will generate code that will support
distributed-memory parallel (e.g. MPI) execution of the model,
including support for halo swaps. A {\tt -nodm} PSyclone option will
cause PSyclone to generate code for serial execution without
distributed memory parallelism. As noted above, each {\tt invoke} call
in the algorithm layer can include one or more operations, and will
result in the generation of one subroutine. So a given PSy layer
subroutine may call many kernels and built-in operations.

For kernel references in the {\tt invoke} call, PSyclone will generate
calls to that particular kernel. For built-in operations supported by
PSyclone like {\tt sum\_X} there is no kernel code: PSyclone generates
all the code required to implement the operation within the PSy layer
subroutine. 

Optional scripts can be written to apply transformations to the
generated code with the aim of optimising the code. For example, the
first widely used script introduced OpenMP parallelism into the main
GungHo dynamics application. The use of scripts makes it easy to add
or remove optimisations without modifying the existing science
code. Scripts are applied at the point of generation of the code that
satisfies a particular algorithm {\tt invoke}. Some scripts (such as
the OpenMP script) will be applicable to all algorithm {\tt invoke}
calls. Other more specialised scripts may be specific to subsets of
the {\tt invoke} calls.

It is important to note here that PSyclone should be regarded as a
tool to assist HPC experts. It is envisaged the HPC expert will
profile the code to find compute performance bottlenecks that cannot
be resolved by the standard optimisation choices. Having found these
bottlenecks, the expert will potentially refactor or rewrite the
algorithm or kernel code. But another option would be to apply a
different script specific to the {\tt invoke} call. The latter may be
a better option where the performance improvement is
platform-specific, as it would apply an optimisation without making a
change to underlying code that could be detrimental on another compute
platform.

In addition to generating PSy layer code in accordance with the kernel
metadata and any applied optimisation scripts, PSyclone includes
various error checking and dependency-checking capabilities to ensure
that the generated code is correct. At the most basic level, PSyclone
will insert halo exchanges at points in the code when they are
needed. Where optimisation scripts seek to reorganise the order in
which kernels are executed, PSyclone applies dependency analysis to
ensure any data dependency between kernels is respected.

\subsubsection{PSy layer code generation example\label{sec:psy_example}}

We now return to the example {\tt invoke} call described above to
illustrate the basic operation of PSyclone using snippets of PSy layer
code. The generated code shown here includes support for distributed
memory parallelism, but no other optimisations. The comments included
in the code snippets are all comments that are generated by PSyclone,
as the aim is that the generated code is readable.

The PSyclone generated code needs to pull information out of the
arguments of the {\tt invoke} call to satisfy the requirements of the
kernel metadata and, thereby, the interface to the kernel subroutine.

The arguments to the {\tt compute\_total\_aam\_code} subroutine
discussed above are as follows:

\begin{description}
\item[{\tt nlayers}] is the number of levels in the input field and is
  extracted from the {\tt aam} field object being written to.
  \item[{\tt aam, u, rho}] relate to the three fields in the
    {\tt invoke} call in the algorithm layer. But rather than
    referencing the field objects, they reference the data within the
    objects.
  \item[{\tt chi\_1, chi\_2, chi\_3}] reference the 3-dimensional
    coordinate data.
  \item[{\tt ndf\_chi, ndf\_w2, ndf\_w3}] are the number of dofs per
    cell for each of the three different function spaces of the input
    fields and the coordinate fields. For example, this number would
    be 6 for a lowest order $\mathbb{W}_{2}$ field illustrated in
    Figure~\ref{fig:k0k1w0-w3} e). In the example code in
    Section~\ref{sec:dofs} we used the more meaningful variable name
    {\tt dofs\_per\_cell}.
  \item[{\tt undf\_chi, undf\_w2, undf\_w3}] are the dimensions of the
    whole field data array for fields on each function space. In the
    example code in Section~\ref{sec:dofs} we used the more meaningful
    variable name {\tt total\_dofs}. The {\tt u} stands for
    ``unique'': where dofs are shared between cells, the total
    ``unique'' number of dofs is not a multiple of {\tt ndf} and the
    number of cells.
  \item[{\tt map\_chi, map\_w2, map\_3}] are the dof-maps for the cell
    being operated on for each unique function space. As noted
    previously, they map the dofs in the cell at the first level of
    the input fields.
  \item[{\tt chi\_basis, w2\_basis, w3\_basis}] are the basis functions
    for each unique function space.
  \item[{\tt chi\_diff\_basis}] references the differential basis
    functions for the coordinate field.
  \item[{\tt nqp\_h, nqp\_v, wqp\_h, wqp\_v}] define the quadrature
    points within the cell (the number of points and their weighting,
    in the horizontal and vertical). This quadrature rule has an
    identical number of points {\tt nqp\_h} in both the {\tt x} and
    {\tt y} horizontal direction and, potentially, a different number
    of points {\tt nqp\_v} in the vertical {\tt z} direction. Note
    this is signalled by the naming convention {\tt
      GH\_QUADRATURE\_XYoZ}; the {\tt GH\_QUADRATURE\_XoYoZ} rule
    could have separate numbers and weighting for the {\tt x} and {\tt
      y} direction and so require 6 arguments in total.
\end{description}

Several of the variables are not well-named as their names are not
self-explanatory. However, the same or similar convention is used in
many kernels, so the naming convention should become familiar with
experience of looking at the code.

For each cell, this particular kernel computes an approximate
integration of the input field over the whole cell. It does this by
computing the value of the field at each of the quadrature points
within the cell. The contributions are summed up into a final result
which is stored in the output {\tt aam} field. The {\tt invoke} call
also calls the {\tt sum\_X} built-in to compute a global sum of the
{\tt aam} field.

PSyclone needs to access data and metadata about each argument in the
{\tt invoke} call. Such data and metadata cannot be accessed directly
from the field object as the algorithm layer (which handles fields)
must not be permitted to directly modify the field data. Instead, each
field contains a reference to a \textbf{field proxy} object. The field
proxy provides pointers to data and metadata within the field
object. Its use is permitted in the PSy layer to enable PSyclone to
obtain the data needed, but is not permitted in the algorithm layer.

The use of separate field and field proxy objects is an important part
of enforcing the PSyKAl separation of concerns. Preventing algorithms
accessing the field proxy objects ensures that the role of running and
optimising operations on the data is fully encapsulated within the PSy
layer: algorithms cannot modify field data in such a way as to
confound the assumptions made when PSyclone generates the PSy layer
code, as all requests must go through the PSy layer via an {\tt
  invoke} call.

Continuing with the example {\tt invoke} call used above, the
generated PSy layer subroutine interface call will be named after the
{\tt name} argument, prefixed by {\tt invoke}, and will take all the
arguments of the kernels and built-ins that were referenced:

\begin{verbatim}
   subroutine invoke_compute_axial_momentum(aam, u, rho, chi, total_aam, qr)
\end{verbatim}

For the four field arguments it first obtains the related proxy
objects that are used to access all the data about the field.

\begin{verbatim}
      ! Initialise field and/or operator proxies
      aam_proxy = aam%get_proxy()
      u_proxy = u%get_proxy()
      rho_proxy = rho%get_proxy()
      chi_proxy(1) = chi(1)%get_proxy()
      chi_proxy(2) = chi(2)%get_proxy()
      chi_proxy(3) = chi(3)%get_proxy()
\end{verbatim}

From now on, all information about the field comes from the proxy
object. Currently, PSyclone assumes that all LFRic fields have the
same number of layers. The number of layers is obtained from the
first field that appeared in the {\tt invoke} call:

\begin{verbatim}
      ! Initialise number of layers
       nlayers = aam_proxy%vspace%get_nlayers()
\end{verbatim}

The mesh object provides access to information about cells within the
mesh for the local partition. For this {\tt invoke}, all fields have
the same mesh:

\begin{verbatim}
      ! Create a mesh object
      mesh => aam_proxy%vspace%get_mesh()
\end{verbatim}

Each function space has a different dof-map. This kernel requires
three unique dof-maps for the four input fields as {\tt aam} and {\tt
  rho} share the same $\mathbb{W}_{3}$ function space. The following calls
return pointers to 2D arrays providing a cell dof-map for every cell
of the first level of each field.

\begin{verbatim}
      ! Look-up dof-maps for each function space
      map_w2  => u_proxy%vspace%get_whole_dofmap()
      map_w3  => aam_proxy%vspace%get_whole_dofmap()
      map_wchi  => chi_proxy%vspace%get_whole_dofmap()
\end{verbatim}

For each function space, the number of dofs per cell {\tt ndf} is
needed. As we pass the whole of the field data into the kernel, we
also extract {\tt undf}, the total number of dofs in the field (on
this shared memory partition). To illustrate, the code for extracting
these values for just one of the function spaces is shown here.

\begin{verbatim}
      ! Initialise number of DoFs for w3
      ndf_w3 = mass_proxy%vspace%get_ndf()
      undf_w3 = mass_proxy%vspace%get_undf()
\end{verbatim}

The kernel in this {\tt invoke} uses the GungHo finite element method,
which requires the field data to be computed by integrating the basis
functions of the function space within the cell. To do this, basis
functions and differential basis functions are computed at various
locations defined by the quadrature rule referenced by the {\tt qr}
argument. If you are not familiar with the finite element method, the
details do not matter, but as an example, the code generated by
PSyclone for computing one set of basis functions is shown.

\begin{verbatim}
      ! Look-up quadrature variables
      qr_proxy = qr%get_quadrature_proxy()
      np_xy_qr = qr_proxy%np_xy
      np_z_qr = qr_proxy%np_z
      weights_xy_qr => qr_proxy%weights_xy
      weights_z_qr => qr_proxy%weights_z

      ! Allocate basis/diff-basis arrays
      dim_w3 = mass_proxy%vspace%get_dim_space()
      allocate (basis_w3_qr(dim_w3, ndf_w3, np_xy_qr, np_z_qr))

      ! Compute basis/diff-basis arrays
      call qr%compute_function(BASIS, aam_proxy%vspace, dim_w3, ndf_w3, basis_w3_qr)
\end{verbatim}

This completes almost all the work to access information required to
call the kernel. 

PSyclone generates a loop over cells to call the kernel, obtaining the
number of cells from the mesh. (Note that unlike the above code
snippets, the kernel call line has been re-formatted for
readability). The final set of information from the proxy is the
actual field data which are referenced directly from the proxy.

\begin{verbatim}
      ! Call kernels and communication routines
      do cell=1,mesh%get_last_edge_cell()
        call compute_total_aam_code(                                  &
             nlayers,                                                 &
             aam_proxy%data, u_proxy%data, rho_proxy%data,            &
             chi_proxy(1)%data, chi_proxy(2)%data, chi_proxy(3)%data, &
             ndf_w3, undf_w3, map_w3(:,cell), basis_w3_qr,            &
             ndf_w2, undf_w2, map_w2(:,cell), basis_w2_qr             &
             ndf_w0_chi, undf_w0_chi, map_w0_chi(:,cell),             &
             basis_w0_chi_qr, diff_basis_w0_chi_qr,                   &
             np_xy_qr, np_z_qr, weights_xy_qr, weights_z_qr)
      end do
 \end{verbatim}

The loop range for this kernel call runs over the local domain
excluding any halo cells. This means that after the loop has
completed, the halo region of the output field is now out of
date. PSyclone signals this by marking the halo as ``dirty'':

\begin{verbatim}
      ! Set halos dirty/clean for fields modified in the above loop
      call aam_proxy%set_dirty()
\end{verbatim}

Should the {\tt aam} field be passed into a subsequent kernel which
requires access to the halo data, PSyclone would identify the
requirement for clean halos from the metadata for that
kernel. PSyclone would insert the following code into the PSy layer to
ensure that the the halos are updated prior to the kernel call (in
this case, to the depth of one halo cell):

\begin{verbatim}
      if (aam_proxy%is_dirty(depth=1)) then
        call aam_proxy%halo_exchange(depth=1)
      end if
\end{verbatim}

The last part of the {\tt invoke} introduced on
Page~\pageref{page:invoke} includes a summation of the {\tt aam} field
output from the kernel. The summation is executed by reference to a
built-in {\tt sum\_X} call. PSyclone generates within the PSy layer
code the following loop over all dofs to compute the local sum, then
uses the LFRic {\tt scalar\_type} API to sum contributions from all
distributed memory regions.

\begin{verbatim}
      use scalar_mod, only: scalar_type
      <snip>
      type(scalar_type) :: global_sum

      ! Zero summation variables
      !
      total_aam = 0.0_r_def
      !
      do df=1,aam_proxy%vspace%get_last_dof_owned()
        total_aam = total_aam+aam_proxy%data(df)
      end do
      global_sum%value = total_aam
      total_aam = global_sum%get_sum()
\end{verbatim}

Finally, the PSy layer routine included allocation of several
variables. PSyclone will generate any necessary deallocate statements.

% TODO For an example of what PSy layer code can look like when OpenMP
% TODO transformations are applied, see Section~\ref{sec:shared_mem_par}.

\subsection{Limitations on numbers of levels}

Currently, kernel metadata does not support the definition of the
number of levels for all the fields passed into the kernel. The kernel
is passed only the number of layers in the first field in the argument
list of the first kernel in an {\tt invoke} subroutine.

Furthermore, if there are two fields with the same function space type
(e.g. $\mathbb{W}_{3}$) but different numbers of levels, then they
cannot both be declared as $\mathbb{W}_{3}$ fields in the kernel
metadata.

The UM boundary layer kernels are an example where this problem needs
to be solved. To ensure the kernel is told the correct number of
levels, the first field in the kernel must be a full-level field. To
differentiate between $\mathbb{W}_{3}$ full-level and $\mathbb{W}_{3}$
single level fields (used for holding surface data) the latter are
declared as one of several dummy any-space function spaces {\tt
  ANY\_DISCONTINUOUS\_SPACE\_1}. PSyclone supports several any-space
settings which can support kernels which can operate on different
types of function space but are also useful in these scenarios. By
using an any-space function space, it ensures that PSyclone extracts a
separate dof-map for these fields.

\section{Building LFRic applications\label{sec:miniapps}}

This section provides an overview of how a whole LFRic application is
constructed, configured and controlled. It uses the GungHo dynamics
and LFRic atmosphere applications to provide examples as these are the
most up to date LFRic applications. It aims to describe rather than
prescribe the way applications are written: not all applications will
have identical requirements, and there may be more than one way to
deliver some application requirements.

The section briefly introduces some additional LFRic capabilities.

\subsection{The top-level program}

The top-level program of all the LFRic applications take as an
argument the name of a namelist file containing all the configuration
options for the application. The top-level program aims to be very
simple. It calls the initialise, run and finalise stages of the
application in sequence, passing the name of the namelist file into
the initialise section.

\subsection{The top-level driver module}

The top-level driver module of the LFRic atmosphere model holds the
initial, run and finalise subroutines of the application. 

Each of the three stages include both core infrastructure operations
and operations relating to the scientific or technical operation of
the model. A well-designed application needs to maintain the
separation of concerns between these two aspects, and the descriptions
of each stage that follow also aim to maintain that separation.

\subsubsection{initial}

The {\tt initial} subroutine is responsible for core technical and
infrastructure tasks including the following:

\begin{itemize}
\item Setting up MPI communicators for the model and for the IO
  subsystem which uses XIOS.
\item Setting up the logging system for outputting text messages.
\item Reading the namelist configuration from the input namelist.
\item Reading in the input meshes and generating the required mesh
  data structures.
\item Initialising the finite element method specifics such as
  function spaces and the chi field.
\item Setting up the XIOS contexts which are needed for each type of
  field that will be read or written, and the XIOS calendar.
\item Calling the science initialisation routines. This includes an
  optional call to output some initial science diagnostics.
\end{itemize}

The science initialisation process is divided into several individual
stages, and includes allocating all the fields that need to exist
throughout the model run, and calling various routines to initialise
fields and settings that will be used throughout the model run.

\subsubsection{Run}

The {\tt run} stage contains the time-step loop for the application
which will include calls both to science code and to infrastructure
code. 

Currently included within the time-step loop are:

\begin{itemize}
\item Calls to update the model calendar based on the time-step
\item A call to a model science step routine which will run the
  science to increment the model state by one time-step.
\item Periodic calls to the diagnostic system to output any required
  diagnostic fields.
\end{itemize}

In the future, additions may include calls to read time-varying
ancillary files.

The science run stage should run a single time-step of the
model. Having the time-step loop in the application run-stage,
separate from the function that increments the science, allows the
science run stage to be more easily called in different ways, such as
by ensemble model systems.

\subsubsection{finalise}

The {\tt finalise} stage tidies up by writing final checkpoint dumps,
outputting any statistics collected during the run, calling finalise
routines for the science components (to deallocate data) and
finalising the logging system and MPI communicators.


\subsection{Fields and field collections\label{sec:fields_intro}}

Two of the main data structures used by a scientist writing LFRic
models and algorithms are fields and field collections. As has been
discussed in Section~\ref{sec:function_spaces_intro}, a field can use
one of several types of function spaces. To create a field on a
particular function space, first the function space is created then
the field initialised.

\begin{verbatim}
! We want to create a W2 function space
use fs_continuity_mod, only : W2

<snip>

type(function_space_type), pointer :: vector_space => null()
type(field_type)                   :: wind_field

! Get a reference to a lowest order W2 function space
vector_space => function_space_collection%get_fs(mesh_id, 0, W2)

! Create a field with a name
call wind_field%initialise(vector_space, name = "wind")
\end{verbatim}

Such a field can then be passed into an algorithm or built-in as
described in Section~\ref{sec:psykal}.

% TODO More technical details can be found for function spaces in
% TODO Section~\ref{sec:function_spaces} and for fields in
% TODO Section~\ref{sec:fields}.

\subsubsection{Field collections and the Depository\label{sec:field_collections_intro}}

Fields can be grouped together in field collections to help modularise
a large number of fields used by a large application. The objects that
can be held within a field collection include any type of field or a
pointer to a field. A field pointer within one collection can point to
an actual field which is held in another collection.

While any algorithm can create a local field collection for its own
purposes, within the LFRic atmosphere fields are organised in a number
of key field collections, illustrated in
Figure~\ref{fig:field_collections}, which are set up in the initialisation
stage and kept in scope throughout the model run:
\begin{itemize}
  \item The top-level driver of the model defines a field collection
    called the Depository during initialisation. The Depository
    contains all the fields that need to be in scope throughout the
    model run.
  \item The top-level driver creates several field collections for
    holding different subsets of field variables required by different
    parts of the model, such as collections of soil, snow and aerosol
    fields. The variables held within these collections are pointers
    to the actual fields in the depository.
  \item In particular, a prognostic field collection is maintained
    which includes pointers to any field that may need to be stored in
    a checkpoint dump.
\end{itemize}

Within the LFRic atmosphere application, grouping fields together in
collections keeps argument lists to the top-level science algorithms
short.

The field collection object supports useful functions such as the
ability to loop through all fields in a collection, for example to
apply the same operation to each field in turn. Within a collection,
all fields are required to have a unique name which acts as the key to
obtain the field.

% TODO More technical details can be found for field collections in
% TODO Section~\ref{sec:field_collections}.

\begin{figure}
\centering
\resizebox{0.66\linewidth}{!}{
\includegraphics*{field_collections}}
\caption{Representation of some field collections as used in the LFRic
  atmosphere implementation. Each blue line represents a single field
  collection. Multiple stacked ovals represent several related fields
  within a field collection that may be referenced by another field
  collection. Each field collection is represented as a single
  variable so can be passed around the model without needing a long
  argument list. In the LFRic atmosphere, the depository field
  collection holds all ``physics'' fields passed into the top-level of
  the model. Other field collections contain pointers to the fields in
  the depository. The prognostic field collection comprises fields
  that need to be written to the model's checkpoint dump. The ``rad''
  and ``soil'' field collections hold pointers to a subset of fields
  relating to radiation and soil, meaning the interface to, say,
  Socrates, does not need to list a lot of individual fields.}
\label{fig:field_collections}
\end{figure}

\subsection{Start dumps and Checkpoint/restart}

Start dumps and checkpoint dumps are two separate types of file that
can be used to initiate a model run of the LFRic atmosphere. For
technical reasons, currently they are different both in terms of file
format and content. Eventually, the formats will converge to be the
same.

Both types of files are UGRID NetCDF files. The start dump format is a
step towards a full UGRID NetCDF file format in which fields are held
as one or multiple two-dimensional layers of data on the model's
unstructured mesh. The checkpoint/restart format is an interim format
which contains raw dumps of the full 3D fields. The reason for the
difference is that the IO infrastructure for writing the start-dump
format does not currently support all the field types required for
checkpoint/restart.

Checkpoint dumps are written at the end of each model run
section. They are read in at the start of the next model run
section. Checkpoint dumps contain a dump of all the fields in the the
prognostic field collection that have been marked as needing to be
checkpointed for a given configuration.

The checkpoint and restart process does not affect model results. Many
applications are required to reproduce the same answers each time they
are run. For example, if one wanted to debug a problem that occurred
on day 25 of a 30 day run, rather than rerunning the whole 30-day job
it would be more efficient to re-run 25 days, and then debug a run
starting from the day-25 checkpoint dump.

\subsection{The diagnostic system}

Fields can be configured to be written to diagnostic output files
periodically during a model run. LFRic supports output of diagnostics
to UGRID files using XIOS \cite{xios}. XIOS uses an input file. {\tt
  iodef.xml} to configure the IO configuration.

Currently, a basic, hard-wired set of diagnostics are available as an
option. A more comprehensive diagnostic system is under development.

\subsection{Time-stepping}

The model configuration defines the start and end time-step and the
time-step length. The start time-step should be aligned with the input
file: either the start dump if the start time-step is 1 or a
checkpoint dump for higher time-step numbers.

A clock object, under development, will gradually replace control by
raw time-step values with the ability to use a range of calendars such
as the Gregorian calendar or the 360-day calendar.

\subsection{Application configuration and the configurator}

The LFRic infrastructure includes a method for defining Fortran
namelists and their requirements within metadata files. A tool called
the \textbf{Configurator} reads these metadata files and generates
Fortran code that defines appropriate namelists and the code to read
in the namelist configuration files. The configuration choices are
accessed in a user-friendly way from a Fortran module.

The metadata specification is aligned with Rose metadata so that the
{\tt rose edit} tool can be used to configure the application. For
example, the following describes configuration of a variable called
{\tt preconditioner}, which can be set to one of several discrete
options (called an enumeration). Each option has a descriptive name
(used within the rose edit application) and the name of a value that
is used to create a variable in the code:

\begin{verbatim}
[namelist:helmholtz_solver=preconditioner]
!enumeration=true
value-titles=None, Diagonal, Tridiagonal, Multigrid
values='none', 'diagonal', 'tridiagonal', 'multigrid'
\end{verbatim}

A user can use {\tt rose edit} to select one of the options in the
{\tt values} list. If the option {\tt tridiagonal} is chosen the
following would appear in the Rose configuration.

\begin{verbatim}
[namelist:helmholtz_solver]
preconditioner='tridiagonal'
\end{verbatim}

In a straightforward manner, this configuration is converted into the
following namelist string variable prior to running the model:

\begin{verbatim}
&helmholtz_solver
preconditioner='tridiagonal',
/
\end{verbatim}

The application developer does not use this namelist variable
directly. The Configurator application generates a namelist definition
and the code to read the namelist. In addition to reading the raw
namelist, the Configurator code will create configuration modules with
meaningful variables. Instead of referencing strings in the code,
named enumerations will be created that are easy to use naturally
within the code.

For example, based on the above definitions, the scientist can use
variables from a configuration module in code as follows. 

\begin{verbatim}
  use helmholtz_solver_config_mod, &
                             only: preconditioner,              &
                                   preconditioner_tridiagonal,  &
                                   preconditioner_multigrid
 \end{verbatim}

The {\tt preconditioner} variable is the variable that is set to one
of the available options. The {\tt preconditioner\_tridiagonal} and
{\tt preconditioner\_multigrid} are variables set to values indicating
two of the valid options for the {\tt preconditioner} variable.

Having obtained the variables from the module, the scientist can write
logical checks to check which particular configuration of the {\tt
  preconditioner} variable the user has selected:

\begin{verbatim}
    if ( preconditioner == preconditioner_tridiagonal ) then
       call tri_precon(1,level)%copy_field(Helm_diag(level))
       ...
    end if
 \end{verbatim}

This approach results in the code containing variables whose names
describe the option they represent instead of using meaningless
integer values.

\subsection{Summary}

This document aimed to provide an overview of the LFRic library and
PSyclone, the PSyKAl architecture that has driven their design and
which underpins the structure of scientific applications that are
written within the LFRic infrastructure.

The core data model was described in which full fields are accessible
at the algorithm layer, and chunks of data from the field are
accessible at the kernel layer. The current standard chunk of a field
is a vertical column of data containing all data points attached to a
column of 3D mesh cells. PSyclone generates the code that loops over
the global data, passing chunks of data to the kernels to ensure the
full field is operated upon correctly. PSyclone manages both the
shared memory parallelism of this operation (such as applying OpenMP
to the loop over columns), and also the distributed memory parallelism
(by creating loops that loop into halos, where required, and inserting
halo exchanges).

The structure of an LFRic application was described, comprising a
driver layer with initialise stage to set up the main prognostic
fields and configuration options, a run stage to run a single
time-step and a finalise stage to undertake any final operations that
may be required.

In focusing on the key aspects required to introduce LFRic to a new
scientist or developer, details of the LFRic implementation have been
kept to a minimum.


\begin{thebibliography}{1}

\bibitem{gunghocs}
  R.~Ford et al.
  \emph{GungHo Phase 1: Computational Science Recommendations
A general method for modeling on irregular grids},\\
  Met Office Forecasting Research Technical Report No: 587,
  2013.

\bibitem{psyclone}
  R.~Ford et al.
  \emph{PSyclone Documentation},\\
https://psyclone.readthedocs.io/en/stable/

\bibitem{umdp15}
  Unified Model Documentation Paper 15:
  \emph{New Dynamics Formulation},\\
https://code.metoffice.gov.uk/doc/um/latest/umdp.html

\bibitem{lfric_wikidocs}
Various LFRic documents and links to documentation, \\
https://code.metoffice.gov.uk/trac/lfric/wiki/LFRicDocumentationPapers

\bibitem{xios}
https://forge.ipsl.jussieu.fr/ioserver

\bibitem{distmem_impl} 
\emph{The implementation of distributed memory parallelism in the
  LFRic Infrastructure}\\
https://code.metoffice.gov.uk/trac/lfric/wiki/LFRicDocumentationPapers


\end{thebibliography}

\end{document}
